# -*- coding: utf-8 -*-
"""AygazMLAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVxhe08jk7FnWDr36fCaTSfrjhFZoOW6
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier,DecisionTreeClassifier,DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import BaggingRegressor

from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.svm import SVR


from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,confusion_matrix,roc_curve,roc_auc_score,auc

from tensorflow.keras.utils import to_categorical
import tensorflow as tf

import warnings
warnings.filterwarnings("ignore")
#Toplantıya katıldım ve youtubedeki webinarlara katılabildim ancak Bayramdan ve üniversite sınavlarına çalışmaktan proje yapmaktan çok vakit bulamadım ama webinardan öğrendiğim bilgiler
#dahilinde ve kendi bildiği kadarıyla kendimce bir şeyler uğraştım olmuştur veya olmamıştır eksik bilgiler vardır ama ben bu süreçte kendime ne kattığıma baktığımdan dolayı her öğrendiğim bilgi benim için nimettir.
#Program için teşekkür ediyorum.
#Neredeyse tüm regresyonlar modeller oluşturuldu veri işleme yapıldı veri görselleştirme yapıldı Standardizasyon Normalizasyon işlemleri yapıldı Korelasyonlar karmaşıklık matrisleri oluşturuldu
#MERİÇ ŞİMŞEK mericsimsek344@gmail.com

dia=pd.read_csv("diabetes.csv")
df=dia.copy()
df.head()

df.isnull().values.any() #hiç eksik değer var mı  (hayır çıktı) evet çıksaydı ona göre davranışla yaklaşacaktık ortalama ile doldurma veya 0 ile doldurma gibi...

df[["Insulin","Age"]].corr() #İnsulin ile yaş arasındaki ilişki yani korelasyonu gösterir.

corr=df.corr()

sns.heatmap(corr,annot=True,linewidths=5)

sns.barplot(x="Insulin",y="Age",data=df)

sns.catplot(x="Insulin",y="Age",data=df)

sns.pairplot(df,hue="Outcome")

sns.pairplot(df,kind ="reg") #Regresyon

sns.lineplot(x="Age",y="Insulin",data=df)

import researchpy as rp
rp.summary_cont(df[["Insulin","BMI","Age"]]) #güven aralığı ortalama gibi değerlere ulaşılır.

import missingno as msno
msno.matrix(df)  #Normalde eksik değerlerin rassallığını yapısal mı değil mi bunu inceleriz ama eksik değerimiz yoktur.

from sklearn import preprocessing

preprocessing.scale(df) #Tüm değişkenler standarlaştırmış oldu tüm hepsi aynı oranda değiştiği için yapıları değişti bu işleme standardizasyon işlemi denilir.

preprocessing.normalize(df)#şimdi normalizasyon işlemine geçeceğiz 0 ile 1 arasına dönüştürmek için kullanılır

sclr=preprocessing.MinMaxScaler(feature_range=(10,20))#Kendi istediğimiz değerler arasında dönüştürme işlemi yapıldı
sclr.fit_transform(df)

df["BMI_Age"]=df["BMI"]*df["Age"]
df["Age_Category"]=pd.cut(df["Age"],bins=[0,20,40,60,np.inf],labels=["0-20","20-40","40-60","60+"])
df["Glucosse_Category"]=pd.cut(df["Glucose"],bins=[0,70,100,125,np.inf],labels=["Low","Normal","Pre-diabetic","Diabetic"])

#bu yukarıda oluşturduğum yeni değişkenler 2 değişkeni bi arada göstermeye yardımcı olabilir mesela
#age category yaşın diyabet riski üzerindeki etkisini daha net bir şekilde analiz etmeye yardımcı olabilir...
df["Glucosse_Category"]

df["Outcome"].value_counts().plot.barh() #şeker hastası olup olmaması sayısı

from sklearn.model_selection import train_test_split
X=df.drop(["Outcome"],axis=1) #Bağımsız değişkenimiz
y=df["Outcome"] #Bağımlı değişkenimiz

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)#Eğitim ve Test setleri oluşturuldu.

import pandas as pd

import statsmodels.api as sm
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

loj = sm.Logit(y_train, X_train) #anlamlılığı r square değerlerini gösterir
loj_model = loj.fit()
print(loj_model.summary())

y_pred=loj.predict(X_test)
y_pred

confusion_matrix(y_test,y_pred) #Karmaşıklık matrisi yani burada sınıflandırma işlemleri olur hastalık taşıyor mu taşımıyormu borcunu ödedi mi ödemedi mi  0 1
#gerçekte 1 iken 1 olan(122)  gerçekte 0 iken 1 denilenler (29)...

accuracy_score(y_test,y_pred) #Doğru sınıflandırma oranımız yani ilkel skorumuz

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred)) #f1 skorumuz  geri dönüş skoru doğru sınıflandırma oranı  ağırlık...

mnist=tf.keras.datasets.mnist

(X_train,y_train),(X_test,y_test) = mnist.load_data()

X_train.shape,y_train.shape #Boyut olarak 28 28 olan 60000 tane eğitim seti var

plt.figure()
plt.imshow(X_train[0])
plt.colorbar()

plt.figure(figsize=(10,10))
for i in range(42):
  plt.subplot(6,7,i+1)
  plt.imshow(X_train[i],cmap=plt.cm.binary)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.title(y_train[i])
  plt.axis("on")

# Verileri 28x28 görüntü formatından 784 özellikli vektörlere düzleştirelim yani farklı bir yapıya evriltiyoruz
X_train=X_train.reshape(X_train.shape[0] , -1)
X_test = X_test.reshape(X_test.shape[0], -1)
# ml de  modeller verileri tek boyutlu vektörler olarak alır. 28*28 boyutundaki görüntü 784 özellikli vektöre dönüştürülür. (28*28=784)

X_train.shape,y_train.shape #X bağımsız y bağımlıdır

loj=LogisticRegression(solver=("liblinear")) #model oluştu
loj_model=loj.fit(X_train,y_train)
y_pred=loj_model.predict(X_test)

loj_model.predict(X)[0:5] #ben sadece 0 ve 1 olasılıklarını isterim

loj_model.predict_proba(X)[0:10][:,0:2] #baştaki sütun 0 sınıfına aittir   1 e ait sınıflar sağ taraftakiler
#mesela 2.si 1 olma olasılığı çok düşük 0.10 yani 0dır   3.sü mesela 1 olma olasılığı yüksek
#mesela 5. de tahmin edilen değerin 1 olması gerekin %74 oranı var ama gerçek değeri 0 çıkmış

y[0:10] #gerçek değerler

loj_model.predict(X_test)[0:10] #tahmin edilen değerler mesela son 2 değeri doğru tahmin etmişiz

from sklearn.metrics import mean_squared_error,r2_score
knn_mdl=KNeighborsClassifier().fit(X_train,y_train)
y_pred = knn_mdl.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred)) #hata kareler ortalaması

accuracy_score(y_test,y_pred)#Doğru sınıflandırma oranımız bu çıktı

print(classification_report(y_test,y_pred)) #f1 skorumuz  geri dönüş skoru doğru sınıflandırma oranı  ağırlık

cm = confusion_matrix(y_test, y_pred) #karmaşıklık matrisimiz
cm

# görsel hali
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True,fmt="d", cmap="Reds", xticklabels=np.arange(2), yticklabels=np.arange(2))
plt.xlabel('Tahmin Edilen Sınıf')
plt.ylabel('Gerçek Sınıf')
plt.title('Karmaşıklık Matrisi')
plt.show()

#Random Forest
rf=RandomForestClassifier().fit(X_train,y_train)
y_pred=rf.predict(X_test)

accuracy_score(y_test,y_pred) #Doğru sınıflandırmamız yani ilkel skorumuz knn e göre daha fazla çıktı

print(classification_report(y_test,y_pred))

print(f'F1 Skoru: {f1_score(y_test,y_pred):.5f}')
print(f'Geri Çağırma Skoru (Recall): {recall_score(y_test,y_pred):.5f}')
print(f'Kesinlik Skoru (Precision): {precision_score(y_test,y_pred):.5f}')

#CART
cart_mdl=DecisionTreeRegressor(max_leaf_nodes=10,min_samples_split=10) #bu değiştikçe bölünmeler gerçekleşir
#mesela tahmini train yapıp max leafi kaldırıp min samplese 2 yapsam  mse o kadar düşüyor ki   2 yaptığımda artık
#giderek 2 tane iç içe if else komutu oluyor
cart_mdl.fit(X_train,y_train)
y_pred=cart_mdl.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

params={"min_samples_split":range(2,100),
        "max_leaf_nodes":range(2,100)}

cart_mdl_for_cv = DecisionTreeRegressor()

cart_cv=GridSearchCV(cart_mdl_for_cv, params, cv=10).fit(X_train,y_train).best_params_ #en iyi değerleri çıktı gösterip yerleştirecez

#best paramsta  min samples  max leaf değerlerini 76 ve 9 verdi onları yerleştiriyoruz
cart_tune=DecisionTreeRegressor(max_leaf_nodes=9,min_samples_split=76).fit(X_train,y_train)

y_pred=cart_mdl.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred)) #tahmin edilen hata değişti

bag_mdl=BaggingRegressor(bootstrap_features=True)#boots. örenkelrini gözlemlek için
bag_mdl.fit(X_train,y_train)#n_estimores =10  10 tane eleman oluşacak   estimators_   ile bunlara ulaşırız
#estimators_feaatures_  bağımsız değişkenlerine ulaşırz
#bu topluluğun hepsi bir araya getirerek oluşturduğu tahminlerdi   ama şimdi bir kaç ağaca tek te
#k gidip sen ne diyosun ben senin fikrini diğer ağaca sen ne diyosun tek tek bilgilerini almak isteriz

y_pred=bag_mdl.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

bag_params={"estimator_":range(2,20)}
bag_cv=GridSearchCV(bag_mdl,bag_params,cv=10).fit(X_train,y_train).best_params_

#şimdi tune haliyle  çıkan en uygun parametreyi tekrar yazdıracaz  14 çıktı
bag_tune=BaggingRegressor(n_estimators=14 ,random_state=45)
bag_tune.fit(X_test)

y_pred=bag_mdl.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred)) #tahmin edilen hata azaldı

#Destek vektör regresyon
svr_mdl=SVR(kernel="linear").fit(X_train,y_train)
svr_pred=svr_mdl.predict(X_train)#train için tahmşb deperller oluştuk çıkan tahmin değer 403
svr_mdl.intercept_ #bu değer -48 çıkar  (sabit değer )
svr_mdl.coef_ # katsayı  bu değer 4.96 çıkar (b1 katsayısı) ,

y_pred=svr_mdl.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(rmse)

mnist=tf.keras.datasets.mnist
(X_train,y_train),(X_test,y_test) = mnist.load_data()

X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0
#Buraya kadar neredeyse tüm regresyonlar modeller oluşturuldu veri işleme yapıldı veri görselleştirme yapıldı Standardizasyon Normalizasyon işlemleri yapıldı Korelasyonlar karmaşıklık matrisleri oluşturuldu devamını yetiştiremedim
#MERİÇ ŞİMŞEK mericsimsek344@gmail.com

#cnn
from tensorflow.keras.layers import Conv2D, Dense, Flatten, \
                                    Concatenate, GlobalMaxPooling2D, \
                                    MaxPooling2D, GlobalAveragePooling2D, \
                                    AveragePooling2D,Dropout

model=tf.keras.Sequential([
    tf.keras.layers.InputLayer((28,28,1)),
    tf.keras.layers.Rescaling(1/255.0),

    Conv2D(6,2,activation="relu"),
    MaxPooling2D((2,2)), # Havuzlama boyutu (2,2) olarak küçülttük
    Conv2D(12,4,activation="relu"),
    MaxPooling2D((2,2)), # Havuzlama boyutu (2,2) olarak küçültüük
    Conv2D(24,6,activation="relu"),
    # Son MaxPooling2D katmanı kaldırıldı
    Flatten(),

    Dense(10,activation="softmax")
])


model.summary()